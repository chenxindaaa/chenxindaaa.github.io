

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.jpg">
  <link rel="icon" href="/img/icon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="chenxindaaa">
  <meta name="keywords" content="">
  
    <meta name="description" content="线性回归 主要内容包括：  线性回归的基本要素 线性回归模型从零开始的实现 线性回归模型使用pytorch的简洁实现  第一次打卡链接： SoftmaxClassify MultilayerPerceptron TextPrepare LanguageModel RecurrentNeuralNetwork 线性回归的基本要素 模型 为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积">
<meta property="og:type" content="article">
<meta property="og:title" content="Dive into DL: Linear Regression">
<meta property="og:url" content="http://chenxindaaa.com/Programming/DL/PyTorch/Dive%20into%20DL/Dive%20into%20DL-Linear%20Regression/index.html">
<meta property="og:site_name" content="chenxindaaa">
<meta property="og:description" content="线性回归 主要内容包括：  线性回归的基本要素 线性回归模型从零开始的实现 线性回归模型使用pytorch的简洁实现  第一次打卡链接： SoftmaxClassify MultilayerPerceptron TextPrepare LanguageModel RecurrentNeuralNetwork 线性回归的基本要素 模型 为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://chenxindaaa.com/img/article_header/1.jpg">
<meta property="article:published_time" content="2020-02-13T16:00:00.000Z">
<meta property="article:modified_time" content="2023-05-08T07:03:14.407Z">
<meta property="article:author" content="chenxindaaa">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://chenxindaaa.com/img/article_header/1.jpg">
  
  
  
  <title>Dive into DL: Linear Regression - chenxindaaa</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/css/fluid-extension.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"chenxindaaa.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>chenxindaaa&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/publications/">
                <i class="iconfont icon-book"></i>
                publication
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/article_header/12.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Dive into DL: Linear Regression"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        chenxindaaa
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2020-02-14 00:00" pubdate>
          February 14, 2020 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          12k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          99 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Dive into DL: Linear Regression</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="线性回归">线性回归</h2>
<p>主要内容包括：</p>
<ol type="1">
<li><p>线性回归的基本要素</p></li>
<li><p>线性回归模型从零开始的实现</p></li>
<li><p>线性回归模型使用pytorch的简洁实现</p></li>
</ol>
<p>第一次打卡链接： <a target="_blank" rel="noopener" href="https://www.chenxindaaa.com/article/PytorchTotrial1_SoftmaxClassify/">SoftmaxClassify</a></p>
<p><a target="_blank" rel="noopener" href="https://www.chenxindaaa.com/article/PytorchTotrial1_MultilayerPerceptron/">MultilayerPerceptron</a></p>
<p><a target="_blank" rel="noopener" href="https://www.chenxindaaa.com/article/PytorchTotrial2_TextPrepare/">TextPrepare</a></p>
<p><a target="_blank" rel="noopener" href="https://www.chenxindaaa.com/article/PytorchTotrial2_LanguageModel/">LanguageModel</a></p>
<p><a target="_blank" rel="noopener" href="https://www.chenxindaaa.com/article/PytorchTotrial2_RecurrentNeuralNetwork/">RecurrentNeuralNetwork</a></p>
<h2 id="线性回归的基本要素">线性回归的基本要素</h2>
<h3 id="模型">模型</h3>
<p>为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:</p>
<p><span class="math display">\[price=w_{area}⋅area+w_{age}⋅age+b\]</span></p>
<h3 id="数据集">数据集</h3>
<p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。</p>
<h3 id="损失函数">损失函数</h3>
<p><strong>1、<span class="math inline">\(MAE / L1 + MSE / L2\)</span></strong></p>
<p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 <span class="math inline">\(i\)</span> 的样本误差的表达式为</p>
<p><span class="math display">\[l^{(i)}(w,b)=\frac{1}{2}(\hat y^{(i)}−y^{(i)})^2,\]</span></p>
<p><span class="math display">\[L(w,b)=\frac{1}{n}\sum ^n _{i=1}l^{(i)}(w,b)=\frac{1}{n}\sum^n_{i=1}\frac{1}{2}(w^⊤x^{(i)}+b−y^{(i)})^2.\]</span></p>
<p>上面的损失函数属于<span class="math inline">\(MSE/L_2\)</span>损失,令残差<span class="math inline">\(r=f(X)-y\)</span>则<span class="math inline">\(MAE/L_1\)</span>损失为：</p>
<p><span class="math display">\[L_1(r)=|r|\]</span></p>
<p><strong><span class="math inline">\(MSE（L_2损失）\)</span>与<span class="math inline">\(MAE（L_1损失）\)</span>的分析：</strong></p>
<p>简单来说，<em>MSE计算简便，但MAE对异常点有更好的鲁棒性</em>。训练一个机器学习模型时，目标就是找到损失函数达到极小值的点。当预测值等于真实值时，这两种函数都能达到最小。</p>
<ul>
<li><p>分析：MSE对误差取了平方（令<span class="math inline">\(r\)</span>=真实值-预测值），因此若<span class="math inline">\(r\)</span>&gt;1，则MSE会进一步增大误差。如果数据中存在异常点，那么e值就会很大，而e²则会远大于|e|。因此，相对于使用MAE计算损失，使用MSE的模型会赋予异常点更大的权重。用RMSE（即MSE的平方根，同MAE在同一量级中）计算损失的模型会以牺牲了其他样本的误差为代价，朝着减小异常点误差的方向更新。然而这就会降低模型的整体性能。直观上可以这样理解：如果我们最小化MSE来对所有的样本点只给出一个预测值，那么这个值一定是所有目标值的平均值。但如果是最小化MAE，那么这个值，则会是所有样本点目标值的中位数。对异常值而言，中位数比均值更加鲁棒，因此MAE对于异常值也比MSE更稳定。</p></li>
<li><p>如何选择损失函数：如果训练数据被异常点所污染（比如，在训练数据中存在大量错误的反例和正例标记，但是在测试集中没有这个问题）或者异常点代表在商业中很重要的异常情况，并且需要被检测出来，则应选用MSE损失函数。相反，如果只把异常值当作受损数据，则应选用MAE损失函数。 MAE存在一个严重的问题（特别是对于神经网络）：更新的梯度始终相同，也就是说，即使对于很小的损失值，梯度也很大。这样不利于模型的学习。为了解决这个缺陷，可以使用变化的学习率，在损失接近最小值时降低学习率。 MSE在这种情况下的表现就很好，即便使用固定的学习率也可以有效收敛。MSE损失的梯度随损失增大而增大，而损失趋于0时则会减小。这使得在训练结束时，使用MSE模型的结果会更精确。</p></li>
<li><p>总结：处理异常点时，L1损失函数更稳定，但它的导数不连续，因此求解效率较低。L2损失函数对异常点更敏感，但通过令其导数为0，可以得到更稳定的封闭解。 二者兼有的问题是：在某些情况下，上述两种损失函数都不能满足需求。例如，若数据中90%的样本对应的目标值为150，剩下10%在0到30之间。那么使用MAE作为损失函数的模型可能会忽视10%的异常点，而对所有样本的预测值都为150。这是因为模型会按中位数来预测。而使用MSE的模型则会给出很多介于0到30的预测值，因为模型会向异常点偏移。上述两种结果在许多商业场景中都是不可取的。最简单的办法是对目标变量进行变换。而另一种办法则是换一个损失函数。</p></li>
</ul>
<p><strong>2、Huber Loss</strong></p>
<p><span class="math display">\[L_\delta (y, f(x))=
\begin{cases}
\frac{1}{2}(y-f(x))^2  &amp; for|y-f(x)|\leq \delta \\[2ex]
\delta|y-f(x)|-\frac{1}{2}\delta ^ 2 &amp; otherwise
\end{cases}\]</span></p>
<p>Huber损失，平滑的平均绝对误差。Huber损失对数据中的异常点没有平方误差损失那么敏感。它在0也可微分。<em>本质上，Huber损失是绝对误差，只是在误差很小时，就变为平方误差</em>。误差降到多小时变为平方误差由超参数δ（delta）来控制。当Huber损失在 [0-<span class="math inline">\(\delta\)</span>，0+<span class="math inline">\(\delta\)</span>] 之间时，等价为MSE，而在 [<span class="math inline">\(-\infty\)</span>，<span class="math inline">\(\delta\)</span>] 和 [<span class="math inline">\(\delta\)</span>，<span class="math inline">\(+\infty\)</span>] 时为MAE。</p>
<p>这里超参数delta的选择非常重要，因为这决定了对异常点的定义。当残差大于delta，应当采用L1（对较大的异常值不那么敏感）来最小化，而残差小于超参数，则用L2来最小化。</p>
<p><strong>如何选择损失函数</strong>：使用MAE训练神经网络最大的一个问题就是不变的大梯度，这可能导致在使用梯度下降快要结束时，错过了最小点。而对于MSE，梯度会随着损失的减小而减小，使结果更加精确。在这种情况下，Huber损失就非常有用。它会由于梯度的减小而落在最小值附近。比起MSE，它对异常点更加鲁棒。因此，Huber损失结合了MSE和MAE的优点。但是，Huber损失的问题是可能需要不断调整超参数delta。</p>
<p><strong>3、Log-Cosh Loss</strong></p>
<p><span class="math display">\[L(y,y^p)=\sum ^n _{i=1}log(cosh(y_i^p)-y_i)\]</span></p>
<p>Log-cosh损失是另一种应用于回归问题中的，且比L2更平滑的的损失函数。它的计算方式是预测误差的双曲余弦的对数。</p>
<p><strong>优点</strong>：对于较小的<span class="math inline">\(x\)</span>，<span class="math inline">\(log(cosh(x))\)</span>近似等于<span class="math inline">\((x^2)/2\)</span>，对于较大的<span class="math inline">\(x\)</span>，近似等于<span class="math inline">\(abs(x)-log(2)\)</span>。这意味着<span class="math inline">\(‘logcosh’\)</span>基本类似于均方误差，但不易受到异常点的影响。它具有Huber损失所有的优点，但不同于Huber损失的是，Log-cosh二阶处处可微。</p>
<p><strong>如何选择损失函数</strong>：许多机器学习模型如XGBoost，就是采用牛顿法来寻找最优点。而牛顿法就需要求解二阶导数（Hessian）。因此对于诸如XGBoost这类机器学习框架，损失函数的二阶可微是很有必要的。但Log-cosh损失也并非完美，其仍存在某些问题。比如误差很大的话，一阶梯度和Hessian会变成定值，这就导致XGBoost出现缺少分裂点的情况。</p>
<p><strong>4、Quantile Loss</strong></p>
<p><span class="math display">\[L_\gamma(u,y^p)=\sum_{i:y_i&lt;y_i^p}(1-\gamma)|y_i-y_i^p|+\sum_{i:y_i\geq y_i^p}\gamma|y_i-y_i^p|\]</span></p>
<p>许多商业问题的决策通常希望了解预测中的不确定性，<strong>更关注区间预测而不仅是点预测</strong>时，分位数损失函数就很有用。</p>
<p>使用最小二乘回归进行区间预测，基于的假设是残差<span class="math inline">\(（y-\hat{y}）\)</span>是独立变量，且方差保持不变。一旦违背了这条假设，那么线性回归模型就不成立。这时，就可以使用分位数损失和分位数回归，<strong>因为即便对于具有变化方差或非正态分布的残差，基于分位数损失的回归也能给出合理的预测区间。</strong></p>
<p><strong>理解分位数损失函数</strong>：如何选取合适的分位值取决于我们对正误差和反误差的重视程度。损失函数通过分位值（γ）对高估和低估给予不同的惩罚。例如，当分位数损失函数γ=0.25时，对高估的惩罚更大，使得预测值略低于中值。</p>
<h3 id="优化函数---随机梯度下降">优化函数 - 随机梯度下降</h3>
<p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。</p>
<p>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch） B ，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。</p>
<p><span class="math display">\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)\]</span></p>
<p>学习率: η 代表在每次优化中，能够学习的步长的大小 批量大小: B 是小批量计算中的批量大小batch size</p>
<p>总结一下，优化函数的有以下两个步骤：</p>
<ul>
<li>(i)初始化模型参数，一般来说使用随机初始化；</li>
<li>(ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。</li>
</ul>
<h2 id="矢量计算">矢量计算</h2>
<p>在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法。</p>
<ol type="1">
<li>向量相加的一种方法是，将这两个向量按元素逐一做标量加法。</li>
<li>向量相加的另一种方法是，将这两个向量直接做矢量加法。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># init variable a, b as 1000 dimension vector</span><br>n = <span class="hljs-number">1000</span><br>a = torch.ones(n)<br>b = torch.ones(n)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># define a timer class to record time</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Timer</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Record multiple running times.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.times = []<br>        self.start()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">start</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># start the timer</span><br>        self.start_time = time.time()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">stop</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># stop the timer and record time into a list</span><br>        self.times.append(time.time() - self.start_time)<br>        <span class="hljs-keyword">return</span> self.times[-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">avg</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># calculate the average and return</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(self.times)/<span class="hljs-built_in">len</span>(self.times)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sum</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># return the sum of recorded time</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(self.times)<br></code></pre></td></tr></table></figure>
<p>现在我们可以来测试了。首先将两个向量使用for循环按元素逐一做标量加法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">timer = Timer()<br>c = torch.zeros(n)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>    c[i] = a[i] + b[i]<br><span class="hljs-string">&#x27;%.5f sec&#x27;</span> % timer.stop()<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">&#39;0.04067 sec&#39;</code></pre>
<p>另外是使用torch来将两个向量直接做矢量加法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">timer.start()<br>d = a + b<br><span class="hljs-string">&#x27;%.5f sec&#x27;</span> % timer.stop()<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">&#39;0.00000 sec&#39;</code></pre>
<p>结果很明显,后者比前者运算速度更快。因此，我们应该尽可能采用矢量计算，以提升计算效率。</p>
<h2 id="线性回归模型从零开始的实现">线性回归模型从零开始的实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># import packages and modules</span><br>%matplotlib inline<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> IPython <span class="hljs-keyword">import</span> display<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br><span class="hljs-built_in">print</span>(torch.__version__)<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">1.4.0+cpu</code></pre>
<h2 id="生成数据集">生成数据集</h2>
<p>使用线性模型来生成数据集，生成一个1000个样本的数据集，下面是用来生成数据的线性关系： <span class="math display">\[price=w_{area}⋅area+w_{age}⋅age+b\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># set input feature number </span><br>num_inputs = <span class="hljs-number">2</span><br><span class="hljs-comment"># set example number</span><br>num_examples = <span class="hljs-number">1000</span><br><br><span class="hljs-comment"># set true weight and bias in order to generate corresponded label</span><br>true_w = [<span class="hljs-number">2</span>, -<span class="hljs-number">3.4</span>]<br>true_b = <span class="hljs-number">4.2</span><br><br>features = torch.randn(num_examples, num_inputs,<br>                      dtype=torch.float32)<br>labels = true_w[<span class="hljs-number">0</span>] * features[:, <span class="hljs-number">0</span>] + true_w[<span class="hljs-number">1</span>] * features[:, <span class="hljs-number">1</span>] + true_b<br>labels += torch.tensor(np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, size=labels.size()),<br>                       dtype=torch.float32)<br></code></pre></td></tr></table></figure>
<p>使用图像来展示生成的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(features[:, <span class="hljs-number">1</span>].numpy(), labels.numpy(), <span class="hljs-number">1</span>);  <span class="hljs-comment"># plt.scatter(x, y, s=20)</span><br></code></pre></td></tr></table></figure>
<figure>
<img src="https://img-blog.csdnimg.cn/20200214205009619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzE1OTg3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-aHyHVLO6-1581684543305)(output_13_0.png)]" /><figcaption>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-aHyHVLO6-1581684543305)(output_13_0.png)]</figcaption>
</figure>
<h2 id="读取数据集">读取数据集</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_iter</span>(<span class="hljs-params">batch_size, features, labels</span>):<br>    num_examples = <span class="hljs-built_in">len</span>(features)<br>    indices = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(num_examples))<br>    random.shuffle(indices)  <span class="hljs-comment"># random read 10 samples</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_examples, batch_size):<br>        j = torch.LongTensor(indices[i: <span class="hljs-built_in">min</span>(i + batch_size, num_examples)]) <span class="hljs-comment"># the last time may be not enough for a whole batch</span><br>        <span class="hljs-keyword">yield</span>  features.index_select(<span class="hljs-number">0</span>, j), labels.index_select(<span class="hljs-number">0</span>, j)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">10</span><br><br><span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter(batch_size, features, labels):<br>    <span class="hljs-built_in">print</span>(X, <span class="hljs-string">&#x27;\n&#x27;</span>, y)<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure>
<pre><code class="hljs">tensor([[-1.5533, -1.2282],
        [ 0.1451,  1.4294],
        [-0.3247, -0.6582],
        [-0.4311,  1.1138],
        [ 0.2255,  0.4859],
        [ 0.0138,  1.2828],
        [ 1.6966,  1.4811],
        [ 0.6216, -1.3915],
        [-0.1157,  0.6430],
        [ 0.2604,  1.9266]]) 
 tensor([ 5.2722, -0.3629,  5.7908, -0.4465,  2.9890, -0.1313,  2.5523, 10.1736,
         1.7958, -1.8457])</code></pre>
<h2 id="初始化模型参数">初始化模型参数</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.tensor(np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, (num_inputs, <span class="hljs-number">1</span>)), dtype=torch.float32) <span class="hljs-comment"># 正态分布</span><br>b = torch.zeros(<span class="hljs-number">1</span>, dtype=torch.float32)<br><br>w.requires_grad_(requires_grad=<span class="hljs-literal">True</span>)<br>b.requires_grad_(requires_grad=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 需要求梯度</span><br></code></pre></td></tr></table></figure>
<pre><code class="hljs">torch.Size([2, 1])</code></pre>
<h2 id="定义模型">定义模型</h2>
<p>定义用来训练参数的训练模型： <span class="math display">\[price=w_{area}⋅area+w_{age}⋅age+b\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">linreg</span>(<span class="hljs-params">X, w, b</span>):<br>    <span class="hljs-keyword">return</span> torch.mm(X, w) + b  <span class="hljs-comment"># mm矩阵乘法</span><br></code></pre></td></tr></table></figure>
<h2 id="定义损失函数">定义损失函数</h2>
<p>我们使用的是均方误差损失函数： <span class="math display">\[l^{(i)}(w,b)=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">squared_loss</span>(<span class="hljs-params">y_hat, y</span>): <br>    <span class="hljs-keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="hljs-number">2</span> / <span class="hljs-number">2</span>  <span class="hljs-comment"># view把y维度弄成和y帽一样</span><br></code></pre></td></tr></table></figure>
<h2 id="定义优化函数">定义优化函数</h2>
<p>在这里优化函数使用的是小批量随机梯度下降： <span class="math display">\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sgd</span>(<span class="hljs-params">params, lr, batch_size</span>): <br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>        param.data -= lr * param.grad / batch_size <span class="hljs-comment"># ues .data to operate param without gradient track</span><br></code></pre></td></tr></table></figure>
<h2 id="训练">训练</h2>
<p>当数据集、模型、损失函数和优化函数定义完了之后就可来准备进行模型的训练了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># super parameters init</span><br>lr = <span class="hljs-number">0.03</span>  <span class="hljs-comment"># 学习率即步长</span><br>num_epochs = <span class="hljs-number">5</span>  <span class="hljs-comment"># epoch训练5轮</span><br><br>net = linreg  <span class="hljs-comment"># 模型</span><br>loss = squared_loss  <span class="hljs-comment"># 损失函数</span><br><br><span class="hljs-comment"># training</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):  <span class="hljs-comment"># training repeats num_epochs times</span><br>    <span class="hljs-comment"># in each epoch, all the samples in dataset will be used once</span><br>    <br>    <span class="hljs-comment"># X is the feature and y is the label of a batch sample</span><br>    <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter(batch_size, features, labels):<br>        l = loss(net(X, w, b), y).<span class="hljs-built_in">sum</span>()  <br>        <span class="hljs-comment"># calculate the gradient of batch sample loss </span><br>        l.backward()  <br>        <span class="hljs-comment"># using small batch random gradient descent to iter model parameters</span><br>        sgd([w, b], lr, batch_size)  <br>        <span class="hljs-comment"># reset parameter gradient</span><br>        w.grad.data.zero_()<br>        b.grad.data.zero_()<br>    train_l = loss(net(features, w, b), labels)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="hljs-number">1</span>, train_l.mean().item()))<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">epoch 1, loss 0.038881
epoch 2, loss 0.000143
epoch 3, loss 0.000048
epoch 4, loss 0.000048
epoch 5, loss 0.000048</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">w, true_w, b, true_b<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">(tensor([[ 1.9993],
         [-3.3998]], requires_grad=True),
 [2, -3.4],
 tensor([4.2000], requires_grad=True),
 4.2)</code></pre>
<h2 id="线性回归模型使用pytorch的简洁实现">线性回归模型使用pytorch的简洁实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>torch.manual_seed(<span class="hljs-number">1</span>)<br><br><span class="hljs-built_in">print</span>(torch.__version__)<br>torch.set_default_tensor_type(<span class="hljs-string">&#x27;torch.FloatTensor&#x27;</span>)<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">1.4.0+cpu</code></pre>
<h2 id="生成数据集-1">生成数据集</h2>
<p>在这里生成数据集跟从零开始的实现中是完全一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">num_inputs = <span class="hljs-number">2</span><br>num_examples = <span class="hljs-number">1000</span><br><br>true_w = [<span class="hljs-number">2</span>, -<span class="hljs-number">3.4</span>]<br>true_b = <span class="hljs-number">4.2</span><br><br>features = torch.tensor(np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (num_examples, num_inputs)), dtype=torch.<span class="hljs-built_in">float</span>)<br>labels = true_w[<span class="hljs-number">0</span>] * features[:, <span class="hljs-number">0</span>] + true_w[<span class="hljs-number">1</span>] * features[:, <span class="hljs-number">1</span>] + true_b<br>labels += torch.tensor(np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, size=labels.size()), dtype=torch.<span class="hljs-built_in">float</span>)<br></code></pre></td></tr></table></figure>
<h2 id="读取数据集-1">读取数据集</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> Data<br><br>batch_size = <span class="hljs-number">10</span><br><br><span class="hljs-comment"># combine featues and labels of dataset</span><br>dataset = Data.TensorDataset(features, labels)<br><br><span class="hljs-comment"># put dataset into DataLoader</span><br>data_iter = Data.DataLoader(<br>    dataset=dataset,            <span class="hljs-comment"># torch TensorDataset format</span><br>    batch_size=batch_size,      <span class="hljs-comment"># mini batch size</span><br>    shuffle=<span class="hljs-literal">True</span>,               <span class="hljs-comment"># whether shuffle the data or not 是否混淆</span><br>    num_workers=<span class="hljs-number">2</span>,              <span class="hljs-comment"># read data in multithreading 两个线程</span><br>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:<br>    <span class="hljs-built_in">print</span>(X, <span class="hljs-string">&#x27;\n&#x27;</span>, y)<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure>
<pre><code class="hljs">tensor([[ 0.1144, -0.2228],
        [ 0.3022,  0.0972],
        [ 0.2309, -0.1299],
        [ 0.3725,  0.7687],
        [ 0.2221, -0.4771],
        [-0.0137,  0.7123],
        [-1.2542,  0.5133],
        [ 0.8199, -0.1336],
        [ 0.9226, -0.3770],
        [-0.4336,  0.4475]]) 
 tensor([ 5.1815,  4.4663,  5.0981,  2.3221,  6.2576,  1.7511, -0.0654,  6.3130,
         7.3475,  1.8041])</code></pre>
<h2 id="定义模型-1">定义模型</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LinearNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_feature</span>):<br>        <span class="hljs-built_in">super</span>(LinearNet, self).__init__()      <span class="hljs-comment"># call father function to init </span><br>        self.linear = nn.Linear(n_feature, <span class="hljs-number">1</span>)  <span class="hljs-comment"># function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        y = self.linear(x)<br>        <span class="hljs-keyword">return</span> y<br>    <br>net = LinearNet(num_inputs)<br><span class="hljs-built_in">print</span>(net)<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">LinearNet(
  (linear): Linear(in_features=2, out_features=1, bias=True)
)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ways to init a multilayer network</span><br><span class="hljs-comment"># method one</span><br>net = nn.Sequential(<br>    nn.Linear(num_inputs, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># other layers can be added here</span><br>    ) <br><br><span class="hljs-comment"># method two</span><br>net = nn.Sequential()<br>net.add_module(<span class="hljs-string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="hljs-number">1</span>))<br><span class="hljs-comment"># net.add_module ......</span><br><br><span class="hljs-comment"># method three</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br>net = nn.Sequential(OrderedDict([<br>          (<span class="hljs-string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="hljs-number">1</span>))<br>          <span class="hljs-comment"># ......</span><br>        ]))<br><br><span class="hljs-built_in">print</span>(net)<br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">Sequential(
  (linear): Linear(in_features=2, out_features=1, bias=True)
)
Linear(in_features=2, out_features=1, bias=True)</code></pre>
<h2 id="初始化模型参数-1">初始化模型参数</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> init<br><br>init.normal_(net[<span class="hljs-number">0</span>].weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.01</span>)<br>init.constant_(net[<span class="hljs-number">0</span>].bias, val=<span class="hljs-number">0.0</span>)  <span class="hljs-comment"># or you can use `net[0].bias.data.fill_(0)` to modify it directly</span><br></code></pre></td></tr></table></figure>
<pre><code class="hljs">Parameter containing:
tensor([0.], requires_grad=True)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> net.parameters():<br>    <span class="hljs-built_in">print</span>(param)<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">Parameter containing:
tensor([[-0.0142, -0.0161]], requires_grad=True)
Parameter containing:
tensor([0.], requires_grad=True)</code></pre>
<h2 id="定义损失函数-1">定义损失函数</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">loss = nn.MSELoss()    <span class="hljs-comment"># nn built-in squared loss function</span><br>                       <span class="hljs-comment"># function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#x27;mean&#x27;)`</span><br></code></pre></td></tr></table></figure>
<h2 id="定义优化函数-1">定义优化函数</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br>optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.03</span>)   <span class="hljs-comment"># built-in random gradient descent function</span><br><span class="hljs-built_in">print</span>(optimizer)  <span class="hljs-comment"># function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</span><br></code></pre></td></tr></table></figure>
<pre><code class="hljs">SGD (
Parameter Group 0
    dampening: 0
    lr: 0.03
    momentum: 0
    nesterov: False
    weight_decay: 0
)</code></pre>
<h2 id="训练-1">训练</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs = <span class="hljs-number">3</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, num_epochs + <span class="hljs-number">1</span>):<br>    <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:<br>        output = net(X)<br>        l = loss(output, y.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        optimizer.zero_grad() <span class="hljs-comment"># reset gradient, equal to net.zero_grad()</span><br>        l.backward()<br>        optimizer.step()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;epoch %d, loss: %f&#x27;</span> % (epoch, l.item()))<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">epoch 1, loss: 0.000369
epoch 2, loss: 0.000100
epoch 3, loss: 0.000132</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># result comparision</span><br>dense = net[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(true_w, dense.weight.data)<br><span class="hljs-built_in">print</span>(true_b, dense.bias.data)<br></code></pre></td></tr></table></figure>
<pre><code class="hljs">[2, -3.4] tensor([[ 2.0004, -3.4000]])
4.2 tensor([4.1998])</code></pre>
<h2 id="两种实现方式的比较">两种实现方式的比较</h2>
<ol type="1">
<li>从零开始的实现（推荐用来学习）</li>
</ol>
<p>能够更好的理解模型和神经网络底层的原理</p>
<ol start="2" type="1">
<li>使用pytorch的简洁实现</li>
</ol>
<p>能够更加快速地完成模型的设计与实现</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Programming/" class="category-chain-item">Programming</a>
  
  
    <span>></span>
    
  <a href="/categories/Programming/DL/" class="category-chain-item">DL</a>
  
  
    <span>></span>
    
  <a href="/categories/Programming/DL/PyTorch/" class="category-chain-item">PyTorch</a>
  
  

  

  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Dive into DL: Linear Regression</div>
      <div>http://chenxindaaa.com/Programming/DL/PyTorch/Dive into DL/Dive into DL-Linear Regression/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>chenxindaaa</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>February 14, 2020</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
